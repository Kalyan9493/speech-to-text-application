{"ast":null,"code":"var _jsxFileName = \"C:\\\\desktop\\\\Personal Projects\\\\speech-to-text\\\\src\\\\SpeechToText.js\",\n  _s = $RefreshSig$();\nimport React, { useState, useEffect, useRef } from 'react';\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nconst SpeechToText = () => {\n  _s();\n  const [text, setText] = useState([]); // Store transcribed text segments\n  const [isListening, setIsListening] = useState(false); // Toggle listening state\n  const recognitionRef = useRef(null); // Ref to store SpeechRecognition instance\n  const lastTimestampRef = useRef(Date.now()); // Ref to store the last segment's timestamp\n\n  useEffect(() => {\n    // Initialize the SpeechRecognition instance\n    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n    if (SpeechRecognition) {\n      const recog = new SpeechRecognition();\n      recog.continuous = true;\n      recog.interimResults = false; // We're only interested in final results\n      recog.lang = 'en-IN'; // Set to English (India)\n\n      recog.onresult = event => {\n        const currentTime = Date.now();\n        const transcript = event.results[event.resultIndex][0].transcript.trim();\n\n        // Check if there is a gap of more than 1 second between speech segments\n        if (currentTime - lastTimestampRef.current > 1000) {\n          // Add a new line with only the current segment\n          setText(prevText => [...prevText, transcript]);\n        } else {\n          // Append to the last line\n          setText(prevText => {\n            const updatedText = [...prevText];\n            updatedText[updatedText.length - 1] += ' ' + transcript;\n            return updatedText;\n          });\n        }\n        lastTimestampRef.current = currentTime; // Update the timestamp\n      };\n      recog.onerror = event => {\n        console.error('Speech recognition error:', event.error);\n        setIsListening(false);\n      };\n      recog.onend = () => {\n        if (isListening) {\n          recog.start(); // Restart recognition if still listening\n        }\n      };\n      recognitionRef.current = recog; // Store the recognition instance in ref\n    } else {\n      alert('Your browser does not support speech recognition.');\n    }\n  }, [isListening]);\n  const startListening = () => {\n    if (recognitionRef.current) {\n      recognitionRef.current.start();\n      setIsListening(true);\n    }\n  };\n  const stopListening = () => {\n    if (recognitionRef.current) {\n      recognitionRef.current.stop();\n      setIsListening(false);\n    }\n  };\n  return /*#__PURE__*/_jsxDEV(\"div\", {\n    children: [/*#__PURE__*/_jsxDEV(\"button\", {\n      onClick: isListening ? stopListening : startListening,\n      children: isListening ? 'Stop Listening' : 'Start Listening'\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 71,\n      columnNumber: 7\n    }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n      children: text.map((line, index) => /*#__PURE__*/_jsxDEV(\"p\", {\n        children: line\n      }, index, false, {\n        fileName: _jsxFileName,\n        lineNumber: 76,\n        columnNumber: 11\n      }, this))\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 74,\n      columnNumber: 7\n    }, this)]\n  }, void 0, true, {\n    fileName: _jsxFileName,\n    lineNumber: 70,\n    columnNumber: 5\n  }, this);\n};\n_s(SpeechToText, \"JU2IlGgmKmKfJf6/ipDO7M12C5A=\");\n_c = SpeechToText;\nexport default SpeechToText;\nvar _c;\n$RefreshReg$(_c, \"SpeechToText\");","map":{"version":3,"names":["React","useState","useEffect","useRef","jsxDEV","_jsxDEV","SpeechToText","_s","text","setText","isListening","setIsListening","recognitionRef","lastTimestampRef","Date","now","SpeechRecognition","window","webkitSpeechRecognition","recog","continuous","interimResults","lang","onresult","event","currentTime","transcript","results","resultIndex","trim","current","prevText","updatedText","length","onerror","console","error","onend","start","alert","startListening","stopListening","stop","children","onClick","fileName","_jsxFileName","lineNumber","columnNumber","map","line","index","_c","$RefreshReg$"],"sources":["C:/desktop/Personal Projects/speech-to-text/src/SpeechToText.js"],"sourcesContent":["import React, { useState, useEffect, useRef } from 'react';\r\n\r\nconst SpeechToText = () => {\r\n  const [text, setText] = useState([]); // Store transcribed text segments\r\n  const [isListening, setIsListening] = useState(false); // Toggle listening state\r\n  const recognitionRef = useRef(null); // Ref to store SpeechRecognition instance\r\n  const lastTimestampRef = useRef(Date.now()); // Ref to store the last segment's timestamp\r\n\r\n  useEffect(() => {\r\n    // Initialize the SpeechRecognition instance\r\n    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\r\n    if (SpeechRecognition) {\r\n      const recog = new SpeechRecognition();\r\n      recog.continuous = true;\r\n      recog.interimResults = false; // We're only interested in final results\r\n      recog.lang = 'en-IN'; // Set to English (India)\r\n\r\n      recog.onresult = (event) => {\r\n        const currentTime = Date.now();\r\n        const transcript = event.results[event.resultIndex][0].transcript.trim();\r\n\r\n        // Check if there is a gap of more than 1 second between speech segments\r\n        if (currentTime - lastTimestampRef.current > 1000) {\r\n          // Add a new line with only the current segment\r\n          setText(prevText => [...prevText, transcript]);\r\n        } else {\r\n          // Append to the last line\r\n          setText(prevText => {\r\n            const updatedText = [...prevText];\r\n            updatedText[updatedText.length - 1] += ' ' + transcript;\r\n            return updatedText;\r\n          });\r\n        }\r\n\r\n        lastTimestampRef.current = currentTime; // Update the timestamp\r\n      };\r\n\r\n      recog.onerror = (event) => {\r\n        console.error('Speech recognition error:', event.error);\r\n        setIsListening(false);\r\n      };\r\n\r\n      recog.onend = () => {\r\n        if (isListening) {\r\n          recog.start(); // Restart recognition if still listening\r\n        }\r\n      };\r\n\r\n      recognitionRef.current = recog; // Store the recognition instance in ref\r\n    } else {\r\n      alert('Your browser does not support speech recognition.');\r\n    }\r\n  }, [isListening]);\r\n\r\n  const startListening = () => {\r\n    if (recognitionRef.current) {\r\n      recognitionRef.current.start();\r\n      setIsListening(true);\r\n    }\r\n  };\r\n\r\n  const stopListening = () => {\r\n    if (recognitionRef.current) {\r\n      recognitionRef.current.stop();\r\n      setIsListening(false);\r\n    }\r\n  };\r\n\r\n  return (\r\n    <div>\r\n      <button onClick={isListening ? stopListening : startListening}>\r\n        {isListening ? 'Stop Listening' : 'Start Listening'}\r\n      </button>\r\n      <div>\r\n        {text.map((line, index) => (\r\n          <p key={index}>{line}</p>\r\n        ))}\r\n      </div>\r\n    </div>\r\n  );\r\n};\r\n\r\nexport default SpeechToText;\r\n"],"mappings":";;AAAA,OAAOA,KAAK,IAAIC,QAAQ,EAAEC,SAAS,EAAEC,MAAM,QAAQ,OAAO;AAAC,SAAAC,MAAA,IAAAC,OAAA;AAE3D,MAAMC,YAAY,GAAGA,CAAA,KAAM;EAAAC,EAAA;EACzB,MAAM,CAACC,IAAI,EAAEC,OAAO,CAAC,GAAGR,QAAQ,CAAC,EAAE,CAAC,CAAC,CAAC;EACtC,MAAM,CAACS,WAAW,EAAEC,cAAc,CAAC,GAAGV,QAAQ,CAAC,KAAK,CAAC,CAAC,CAAC;EACvD,MAAMW,cAAc,GAAGT,MAAM,CAAC,IAAI,CAAC,CAAC,CAAC;EACrC,MAAMU,gBAAgB,GAAGV,MAAM,CAACW,IAAI,CAACC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC;;EAE7Cb,SAAS,CAAC,MAAM;IACd;IACA,MAAMc,iBAAiB,GAAGC,MAAM,CAACD,iBAAiB,IAAIC,MAAM,CAACC,uBAAuB;IACpF,IAAIF,iBAAiB,EAAE;MACrB,MAAMG,KAAK,GAAG,IAAIH,iBAAiB,CAAC,CAAC;MACrCG,KAAK,CAACC,UAAU,GAAG,IAAI;MACvBD,KAAK,CAACE,cAAc,GAAG,KAAK,CAAC,CAAC;MAC9BF,KAAK,CAACG,IAAI,GAAG,OAAO,CAAC,CAAC;;MAEtBH,KAAK,CAACI,QAAQ,GAAIC,KAAK,IAAK;QAC1B,MAAMC,WAAW,GAAGX,IAAI,CAACC,GAAG,CAAC,CAAC;QAC9B,MAAMW,UAAU,GAAGF,KAAK,CAACG,OAAO,CAACH,KAAK,CAACI,WAAW,CAAC,CAAC,CAAC,CAAC,CAACF,UAAU,CAACG,IAAI,CAAC,CAAC;;QAExE;QACA,IAAIJ,WAAW,GAAGZ,gBAAgB,CAACiB,OAAO,GAAG,IAAI,EAAE;UACjD;UACArB,OAAO,CAACsB,QAAQ,IAAI,CAAC,GAAGA,QAAQ,EAAEL,UAAU,CAAC,CAAC;QAChD,CAAC,MAAM;UACL;UACAjB,OAAO,CAACsB,QAAQ,IAAI;YAClB,MAAMC,WAAW,GAAG,CAAC,GAAGD,QAAQ,CAAC;YACjCC,WAAW,CAACA,WAAW,CAACC,MAAM,GAAG,CAAC,CAAC,IAAI,GAAG,GAAGP,UAAU;YACvD,OAAOM,WAAW;UACpB,CAAC,CAAC;QACJ;QAEAnB,gBAAgB,CAACiB,OAAO,GAAGL,WAAW,CAAC,CAAC;MAC1C,CAAC;MAEDN,KAAK,CAACe,OAAO,GAAIV,KAAK,IAAK;QACzBW,OAAO,CAACC,KAAK,CAAC,2BAA2B,EAAEZ,KAAK,CAACY,KAAK,CAAC;QACvDzB,cAAc,CAAC,KAAK,CAAC;MACvB,CAAC;MAEDQ,KAAK,CAACkB,KAAK,GAAG,MAAM;QAClB,IAAI3B,WAAW,EAAE;UACfS,KAAK,CAACmB,KAAK,CAAC,CAAC,CAAC,CAAC;QACjB;MACF,CAAC;MAED1B,cAAc,CAACkB,OAAO,GAAGX,KAAK,CAAC,CAAC;IAClC,CAAC,MAAM;MACLoB,KAAK,CAAC,mDAAmD,CAAC;IAC5D;EACF,CAAC,EAAE,CAAC7B,WAAW,CAAC,CAAC;EAEjB,MAAM8B,cAAc,GAAGA,CAAA,KAAM;IAC3B,IAAI5B,cAAc,CAACkB,OAAO,EAAE;MAC1BlB,cAAc,CAACkB,OAAO,CAACQ,KAAK,CAAC,CAAC;MAC9B3B,cAAc,CAAC,IAAI,CAAC;IACtB;EACF,CAAC;EAED,MAAM8B,aAAa,GAAGA,CAAA,KAAM;IAC1B,IAAI7B,cAAc,CAACkB,OAAO,EAAE;MAC1BlB,cAAc,CAACkB,OAAO,CAACY,IAAI,CAAC,CAAC;MAC7B/B,cAAc,CAAC,KAAK,CAAC;IACvB;EACF,CAAC;EAED,oBACEN,OAAA;IAAAsC,QAAA,gBACEtC,OAAA;MAAQuC,OAAO,EAAElC,WAAW,GAAG+B,aAAa,GAAGD,cAAe;MAAAG,QAAA,EAC3DjC,WAAW,GAAG,gBAAgB,GAAG;IAAiB;MAAAmC,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OAC7C,CAAC,eACT3C,OAAA;MAAAsC,QAAA,EACGnC,IAAI,CAACyC,GAAG,CAAC,CAACC,IAAI,EAAEC,KAAK,kBACpB9C,OAAA;QAAAsC,QAAA,EAAgBO;MAAI,GAAZC,KAAK;QAAAN,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OAAW,CACzB;IAAC;MAAAH,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OACC,CAAC;EAAA;IAAAH,QAAA,EAAAC,YAAA;IAAAC,UAAA;IAAAC,YAAA;EAAA,OACH,CAAC;AAEV,CAAC;AAACzC,EAAA,CA9EID,YAAY;AAAA8C,EAAA,GAAZ9C,YAAY;AAgFlB,eAAeA,YAAY;AAAC,IAAA8C,EAAA;AAAAC,YAAA,CAAAD,EAAA","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}